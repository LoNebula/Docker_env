{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU PyTorch Demo\n",
    "\n",
    "This notebook demonstrates GPU PyTorch functionality in your development container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix size for performance test\n",
    "size = 2048\n",
    "\n",
    "# CPU computation\n",
    "x_cpu = torch.randn(size, size)\n",
    "y_cpu = torch.randn(size, size)\n",
    "\n",
    "start_time = time.time()\n",
    "z_cpu = torch.matmul(x_cpu, y_cpu)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "print(f\"CPU computation time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# GPU computation (if available)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x_gpu = torch.randn(size, size, device=device)\n",
    "    y_gpu = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    _ = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"GPU computation time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\nelse:\n",
    "    print(\"GPU not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network and move to GPU\n",
    "model = SimpleNet(784, 256, 10).to(device)\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "\n",
    "# Generate some dummy data\n",
    "batch_size = 64\n",
    "input_data = torch.randn(batch_size, 784).to(device)\n",
    "target = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "\n",
    "print(f\"Input data shape: {input_data.shape}\")\n",
    "print(f\"Input data device: {input_data.device}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output device: {output.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nAfter clearing cache:\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\nelse:\n",
    "    print(\"GPU memory information not available (CPU only)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
